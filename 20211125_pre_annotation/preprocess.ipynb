{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuba\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3457: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('source.csv')\n",
    "data.drop_duplicates(subset=['char'], keep='first', inplace=True)\n",
    "data.sort_values(by=['char'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步係要將唔需要嘅詞條都清走擺埋一邊，呢啲詞條包括：\n",
    "\n",
    "1. 字數大於 4 嘅詞條\n",
    "1. 唔常用嘅專名，包括：\n",
    "   1. 街道名路名\n",
    "   1. 城市村寨樓盤名\n",
    "   1. 車站名\n",
    "   1. 商鋪名\n",
    "   1. 機構組織名\n",
    "1. 幾個詞表入面已經有嘅名\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuba\\AppData\\Local\\Temp/ipykernel_27128/179914394.py:3: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  proper_nouns = data[data['char'].str.contains(pattern)]\n",
      "C:\\Users\\liuba\\AppData\\Local\\Temp/ipykernel_27128/179914394.py:4: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  remain = data[~data['char'].str.contains(pattern)]\n"
     ]
    }
   ],
   "source": [
    "pattern = r'(店|路|道|巷|莊|鎮|苑|市|街|村|站|綫|學|校|樓|場|館|社|屋|岭|所|行|園|城|中心|百貨|大廈|屋|房|廳|廠|院|酒吧|網吧|印刷|教育|文具|化工|社區|實業|酒家|科技|餐|廊|公司|女裝|時裝|男裝|電訊)$'\n",
    "\n",
    "proper_nouns = data[data['char'].str.contains(pattern)]\n",
    "remain = data[~data['char'].str.contains(pattern)]\n",
    "\n",
    "long = remain[remain['char'].str.len() > 4]\n",
    "remain = remain[remain['char'].str.len() <= 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然後要將已經有嘅嗰啲數據都清晒佢，包括冚唪唥詞表粵典詞表 CyberCan 詞表等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbl = pd.read_csv('hbl.csv')\n",
    "condition = remain['char'].isin(hbl['char'])\n",
    "remain.drop(remain[condition].index, inplace = True)\n",
    "\n",
    "wordshk = pd.read_csv('wordshk.csv')\n",
    "condition = remain['char'].isin(wordshk['char'])\n",
    "remain.drop(remain[condition].index, inplace = True)\n",
    "\n",
    "cybercan = pd.read_csv('CyberCan.csv')\n",
    "condition = remain['char'].isin(cybercan['Words'])\n",
    "remain.drop(remain[condition].index, inplace = True)\n",
    "\n",
    "# 下面嘅都係ngram，唔可以當詞嚟用\n",
    "apple_daily = pd.read_csv('AppleDaily.csv')\n",
    "condition = remain['char'].isin(apple_daily['ngram'])\n",
    "remain.drop(remain[condition].index, inplace = True)\n",
    "\n",
    "wiki = pd.read_csv('wiki.csv')\n",
    "condition = remain['char'].isin(wiki['ngram'])\n",
    "remain.drop(remain[condition].index, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟住要將啲官話詞抽出嚟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuba\\AppData\\Local\\Temp/ipykernel_27128/2427951999.py:12: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  mandarin = remain[remain['char'].str.contains(mandarin_words)]\n",
      "C:\\Users\\liuba\\AppData\\Local\\Temp/ipykernel_27128/2427951999.py:13: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  remain = remain[~remain['char'].str.contains(mandarin_words)]\n"
     ]
    }
   ],
   "source": [
    "# common_words = pd.concat([hbl, wordshk, cybercan])\n",
    "# common_words.drop_duplicates(subset=['char'], keep='first', inplace=True)\n",
    "\n",
    "# common_two_words = common_words[common_words['char'].str.len() == 2]\n",
    "\n",
    "# combo_words = []\n",
    "# for index, four_word in remain_four.iterrows():\n",
    "#     if any(map(four_word['char'].__contains__,common_two_words['char'])):\n",
    "#       combo_words.append(four_word)\n",
    "\n",
    "mandarin_words = r'(這|那|吧|他|她|它|們|是|的|哪)'\n",
    "mandarin = remain[remain['char'].str.contains(mandarin_words)]\n",
    "remain = remain[~remain['char'].str.contains(mandarin_words)]\n",
    "\n",
    "\n",
    "remain.to_csv('remain.csv', index=False)\n",
    "long.to_csv('remain_long.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然後打亂佢，分成一行一千個詞導出到個文件夾入面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = remain.sample(frac=1)\n",
    "\n",
    "num_slices = shuffled.shape[0] // 1000 + 1\n",
    "\n",
    "for i in range(num_slices):\n",
    "    start = i * 1000\n",
    "    end = (i + 1) * 1000\n",
    "    sliced = shuffled.iloc[start:end]\n",
    "    sliced.to_csv(f'./round_1_raw/r1_{i+1}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
